"""
ğŸš€ Performance Benchmark Script

PorovnÃ¡nÃ­ rychlosti rÅ¯znÃ½ch konfiguracÃ­ faster-whisper.
PouÅ¾itÃ­: uv run benchmark.py <audio_file>
"""

import sys
import time
import json
from faster_whisper import WhisperModel, BatchedInferencePipeline

def benchmark_config(audio_path, config_name, model_size, use_batched, batch_size, beam_size):
    """Benchmarkuje jednu konfiguraci"""
    print(f"\n{'='*60}")
    print(f"ğŸ”¬ Test: {config_name}")
    print(f"{'='*60}")
    
    try:
        # NaÄtenÃ­ modelu
        print(f"â³ NaÄÃ­tÃ¡m model '{model_size}'...")
        model = WhisperModel(model_size, device="cuda", compute_type="float16")
        
        # PÅ™epis
        start_time = time.time()
        
        if use_batched:
            print(f"ğŸ“¦ Batched inference (batch_size={batch_size})")
            batched_model = BatchedInferencePipeline(model=model)
            segments, info = batched_model.transcribe(
                audio_path,
                batch_size=batch_size,
                beam_size=beam_size,
                vad_filter=True
            )
        else:
            print(f"ğŸ“ Sequential inference")
            segments, info = model.transcribe(
                audio_path,
                beam_size=beam_size,
                vad_filter=True
            )
        
        # Konzumace generÃ¡toru
        segment_count = sum(1 for _ in segments)
        
        # VÃ½poÄet metrik
        elapsed = time.time() - start_time
        audio_duration = info.duration
        rtf = audio_duration / elapsed  # Real-Time Factor
        
        print(f"\nâœ… VÃSLEDKY:")
        print(f"   Audio dÃ©lka: {audio_duration:.2f}s")
        print(f"   ÄŒas zpracovÃ¡nÃ­: {elapsed:.2f}s")
        print(f"   RTF: {rtf:.2f}x (vyÅ¡Å¡Ã­ = rychlejÅ¡Ã­)")
        print(f"   SegmentÅ¯: {segment_count}")
        print(f"   Jazyk: {info.language} ({info.language_probability:.1%})")
        
        return {
            "config": config_name,
            "audio_duration": audio_duration,
            "processing_time": elapsed,
            "rtf": rtf,
            "segments": segment_count
        }
        
    except Exception as e:
        print(f"âŒ Chyba: {e}")
        return None

def main():
    if len(sys.argv) < 2:
        print("PouÅ¾itÃ­: uv run benchmark.py <audio_file>")
        sys.exit(1)
    
    audio_path = sys.argv[1]
    
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ¯ FASTER-WHISPER PERFORMANCE BENCHMARK                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Audio soubor: {audio_path}

Tento test porovnÃ¡ rÅ¯znÃ© konfigurace a ukÃ¡Å¾e, kterÃ¡ je
nejrychlejÅ¡Ã­ pro vÃ¡Å¡ hardware.
""")
    
    results = []
    
    # Test 1: Medium + Batched (doporuÄenÃ¡ konfigurace)
    results.append(benchmark_config(
        audio_path,
        "Medium + Batched (DOPORUÄŒENO)",
        "medium",
        use_batched=True,
        batch_size=16,
        beam_size=5
    ))
    
    # Test 2: Medium bez Batched
    results.append(benchmark_config(
        audio_path,
        "Medium bez Batched",
        "medium",
        use_batched=False,
        batch_size=None,
        beam_size=5
    ))
    
    # Test 3: Small + AgresivnÃ­ batching
    results.append(benchmark_config(
        audio_path,
        "Small + VelkÃ½ Batch (RYCHLOST)",
        "small",
        use_batched=True,
        batch_size=32,
        beam_size=3
    ))
    
    # VyhodnocenÃ­
    print(f"\n\n{'='*60}")
    print("ğŸ“Š SOUHRNNÃ‰ VÃSLEDKY")
    print(f"{'='*60}\n")
    
    valid_results = [r for r in results if r is not None]
    if not valid_results:
        print("âŒ Å½Ã¡dnÃ© ÃºspÄ›Å¡nÃ© testy")
        return
    
    # SeÅ™azenÃ­ podle RTF (nejvyÅ¡Å¡Ã­ = nejrychlejÅ¡Ã­)
    valid_results.sort(key=lambda x: x["rtf"], reverse=True)
    
    print(f"{'Konfigurace':<35} {'RTF':>8} {'ÄŒas':>10}")
    print("-" * 60)
    for r in valid_results:
        print(f"{r['config']:<35} {r['rtf']:>7.2f}x {r['processing_time']:>9.2f}s")
    
    print("\nğŸ† VÃTÄšZ:")
    winner = valid_results[0]
    print(f"   {winner['config']}")
    print(f"   Rychlost: {winner['rtf']:.2f}x real-time")
    
    # UloÅ¾enÃ­ vÃ½sledkÅ¯
    with open("benchmark_results.json", "w", encoding="utf-8") as f:
        json.dump(valid_results, f, indent=2, ensure_ascii=False)
    print(f"\nğŸ’¾ DetailnÃ­ vÃ½sledky uloÅ¾eny do: benchmark_results.json")

if __name__ == "__main__":
    main()
